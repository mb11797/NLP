{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenize\n",
    "word_tokens_MB = text.split()\n",
    "word_tokens_nltk = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry.', 'Lorem', 'Ipsum', 'has', 'been', 'the', \"industry's\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s,', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book.', 'It', 'has', 'survived', 'not', 'only', 'five', 'centuries,', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting,', 'remaining', 'essentially', 'unchanged.', 'It', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'Letraset', 'sheets', 'containing', 'Lorem', 'Ipsum', 'passages,', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'Aldus', 'PageMaker', 'including', 'versions', 'of', 'Lorem', 'Ipsum.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens_MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokens_MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry', '.', 'Lorem', 'Ipsum', 'has', 'been', 'the', 'industry', \"'s\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s', ',', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book', '.', 'It', 'has', 'survived', 'not', 'only', 'five', 'centuries', ',', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting', ',', 'remaining', 'essentially', 'unchanged', '.', 'It', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'Letraset', 'sheets', 'containing', 'Lorem', 'Ipsum', 'passages', ',', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'Aldus', 'PageMaker', 'including', 'versions', 'of', 'Lorem', 'Ipsum', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokens_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence tokenize\n",
    "sent_tokens_MB = text.split('.')\n",
    "sent_tokens_nltk = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry',\n",
       " \" Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book\",\n",
       " ' It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged',\n",
       " ' It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum',\n",
       " '']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens_MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry.',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\",\n",
       " 'It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.',\n",
       " 'It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokens_MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_tokens_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = \"Hello how are you. Diff.between me and you. Hope you are doing well. Will try to meet soon. Take care\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_2_tokens_MB = text_2.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello how are you',\n",
       " ' Diff',\n",
       " 'between me and you',\n",
       " ' Hope you are doing well',\n",
       " ' Will try to meet soon',\n",
       " ' Take care']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_2_tokens_MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_2_tokens_nltk = sent_tokenize(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello how are you.',\n",
       " 'Diff.between me and you.',\n",
       " 'Hope you are doing well.',\n",
       " 'Will try to meet soon.',\n",
       " 'Take care']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_2_tokens_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_2_tokens_nltk[0].endswith(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_2_tokens_nltk[0].startswith(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        words.append(word.lower())\n",
    "    return words\n",
    "        \n",
    "def remove_stopwords(words):\n",
    "    final_list = []\n",
    "    for word in words:\n",
    "        if word not in stopwords_list:\n",
    "            final_list.append(word)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenize(text)\n",
    "final_list = remove_stopwords(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lorem', 'ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry.', 'lorem', 'ipsum', 'has', 'been', 'the', \"industry's\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s,', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book.', 'it', 'has', 'survived', 'not', 'only', 'five', 'centuries,', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting,', 'remaining', 'essentially', 'unchanged.', 'it', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'letraset', 'sheets', 'containing', 'lorem', 'ipsum', 'passages,', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'aldus', 'pagemaker', 'including', 'versions', 'of', 'lorem', 'ipsum.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lorem',\n",
       " 'ipsum',\n",
       " 'simply',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'printing',\n",
       " 'typesetting',\n",
       " 'industry.',\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " \"industry's\",\n",
       " 'standard',\n",
       " 'dummy',\n",
       " 'text',\n",
       " 'ever',\n",
       " 'since',\n",
       " '1500s,',\n",
       " 'unknown',\n",
       " 'printer',\n",
       " 'took',\n",
       " 'galley',\n",
       " 'type',\n",
       " 'scrambled',\n",
       " 'make',\n",
       " 'type',\n",
       " 'specimen',\n",
       " 'book.',\n",
       " 'survived',\n",
       " 'five',\n",
       " 'centuries,',\n",
       " 'also',\n",
       " 'leap',\n",
       " 'electronic',\n",
       " 'typesetting,',\n",
       " 'remaining',\n",
       " 'essentially',\n",
       " 'unchanged.',\n",
       " 'popularised',\n",
       " '1960s',\n",
       " 'release',\n",
       " 'letraset',\n",
       " 'sheets',\n",
       " 'containing',\n",
       " 'lorem',\n",
       " 'ipsum',\n",
       " 'passages,',\n",
       " 'recently',\n",
       " 'desktop',\n",
       " 'publishing',\n",
       " 'software',\n",
       " 'like',\n",
       " 'aldus',\n",
       " 'pagemaker',\n",
       " 'including',\n",
       " 'versions',\n",
       " 'lorem',\n",
       " 'ipsum.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(f_list):\n",
    "    tfreq = dict()\n",
    "    for word in f_list:\n",
    "#         print(word)\n",
    "        if word in tfreq:\n",
    "            tfreq[word] += 1\n",
    "        else:\n",
    "            tfreq[word] = 1\n",
    "#             print(tfreq)\n",
    "    \n",
    "    return tfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lorem': 4, 'ipsum': 3, 'simply': 1, 'dummy': 2, 'text': 2, 'printing': 1, 'typesetting': 1, 'industry.': 1, \"industry's\": 1, 'standard': 1, 'ever': 1, 'since': 1, '1500s,': 1, 'unknown': 1, 'printer': 1, 'took': 1, 'galley': 1, 'type': 2, 'scrambled': 1, 'make': 1, 'specimen': 1, 'book.': 1, 'survived': 1, 'five': 1, 'centuries,': 1, 'also': 1, 'leap': 1, 'electronic': 1, 'typesetting,': 1, 'remaining': 1, 'essentially': 1, 'unchanged.': 1, 'popularised': 1, '1960s': 1, 'release': 1, 'letraset': 1, 'sheets': 1, 'containing': 1, 'passages,': 1, 'recently': 1, 'desktop': 1, 'publishing': 1, 'software': 1, 'like': 1, 'aldus': 1, 'pagemaker': 1, 'including': 1, 'versions': 1, 'ipsum.': 1}\n"
     ]
    }
   ],
   "source": [
    "print(tf(final_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(term,doc):\n",
    "    print(\"Divide {} / {}\".format(doc.count(term.lower()),len(doc)))\n",
    "    return doc.count(term.lower()) / len(doc)                     #??????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divide 4 / 57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07017543859649122"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text('lorem',final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divide 4 / 57\n",
      "Normalize for lorem is 0.070\n",
      "Divide 3 / 57\n",
      "Normalize for ipsum is 0.053\n",
      "Divide 1 / 57\n",
      "Normalize for simply is 0.018\n",
      "Divide 2 / 57\n",
      "Normalize for dummy is 0.035\n",
      "Divide 2 / 57\n",
      "Normalize for text is 0.035\n",
      "Divide 1 / 57\n",
      "Normalize for printing is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for typesetting is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for industry. is 0.018\n",
      "Divide 4 / 57\n",
      "Normalize for lorem is 0.070\n",
      "Divide 3 / 57\n",
      "Normalize for ipsum is 0.053\n",
      "Divide 1 / 57\n",
      "Normalize for industry's is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for standard is 0.018\n",
      "Divide 2 / 57\n",
      "Normalize for dummy is 0.035\n",
      "Divide 2 / 57\n",
      "Normalize for text is 0.035\n",
      "Divide 1 / 57\n",
      "Normalize for ever is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for since is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for 1500s, is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for unknown is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for printer is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for took is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for galley is 0.018\n",
      "Divide 2 / 57\n",
      "Normalize for type is 0.035\n",
      "Divide 1 / 57\n",
      "Normalize for scrambled is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for make is 0.018\n",
      "Divide 2 / 57\n",
      "Normalize for type is 0.035\n",
      "Divide 1 / 57\n",
      "Normalize for specimen is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for book. is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for survived is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for five is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for centuries, is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for also is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for leap is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for electronic is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for typesetting, is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for remaining is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for essentially is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for unchanged. is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for popularised is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for 1960s is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for release is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for letraset is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for sheets is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for containing is 0.018\n",
      "Divide 4 / 57\n",
      "Normalize for lorem is 0.070\n",
      "Divide 3 / 57\n",
      "Normalize for ipsum is 0.053\n",
      "Divide 1 / 57\n",
      "Normalize for passages, is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for recently is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for desktop is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for publishing is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for software is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for like is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for aldus is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for pagemaker is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for including is 0.018\n",
      "Divide 1 / 57\n",
      "Normalize for versions is 0.018\n",
      "Divide 4 / 57\n",
      "Normalize for lorem is 0.070\n",
      "Divide 1 / 57\n",
      "Normalize for ipsum. is 0.018\n"
     ]
    }
   ],
   "source": [
    "nm_list = []\n",
    "\n",
    "for term in final_list:\n",
    "    normalize = normalize_text(term,final_list)\n",
    "    print(\"Normalize for %s is %.3f\"%(term,normalize))\n",
    "    nm_list.append(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "document_1 = \"How can i be a good cricketer\"\n",
    "document_2 = \"What should I do to become a good cricketer\"\n",
    "\n",
    "document = [document_1, document_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How can i be a good cricketer',\n",
       " 'What should I do to become a good cricketer']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverseDocFreq(term,all_doc):\n",
    "    numDocWithThisTerm = 0\n",
    "    \n",
    "    for doc in all_doc:\n",
    "        if term.lower() in doc.lower().split():\n",
    "            numDocWithThisTerm += 1\n",
    "        \n",
    "    if numDocWithThisTerm > 0 :\n",
    "        return 1.0 + log(float(len(all_doc)) / numDocWithThisTerm)\n",
    "    else:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverseDocFreq('good',document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6931471805599454"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverseDocFreq('become',document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverseDocFreq('Baap',document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
